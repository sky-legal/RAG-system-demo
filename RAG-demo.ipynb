{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55f088b4-6d85-48d2-8ccd-d981baadb334",
   "metadata": {},
   "source": [
    "# 1 - G√©n√©rer les Embeddings\n",
    "Cette cellule effectue l'importation des biblioth√®ques `os`, `numpy`, `pandas` et `SentenceTransformer`, puis charge le mod√®le de transformation de phrases `all-MiniLM-L6-v2`. Elle lit ensuite le fichier `texte-nettoye.txt` et segmente son contenu en phrases en utilisant le point comme s√©parateur. Apr√®s avoir supprim√© les espaces inutiles et filtr√© les segments vides, elle g√©n√®re des embeddings vectoriels pour chaque phrase √† l‚Äôaide du mod√®le charg√©. Les r√©sultats sont ensuite stock√©s dans un DataFrame associant chaque phrase √† son vecteur d‚Äôembedding, puis enregistr√©s dans un fichier CSV intitul√© `embeddings.csv`. C'est le fichier que nous allons exploiter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9fb4897b-f532-4bba-bb84-fc0979d3e570",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings enregistr√©s dans embeddings.csv :)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "with open(\"texte-nettoye.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    texte = f.read()\n",
    "\n",
    "chunks = texte.split(\".\")  \n",
    "chunks = [chunk.strip() for chunk in chunks if chunk.strip()]\n",
    "\n",
    "embeddings = model.encode(chunks)\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"chunk\": chunks,\n",
    "    \"embedding\": embeddings.tolist()\n",
    "})\n",
    "\n",
    "df.to_csv(\"embeddings.csv\", index=False)\n",
    "\n",
    "print(\"Embeddings enregistr√©s dans embeddings.csv :)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c795f7c6-8445-4c60-8a18-472b0992fa1f",
   "metadata": {},
   "source": [
    "# 2 - Indexation FAISS \n",
    "\n",
    "Ici, nous utilisons la biblioth√®que `faiss` pour optimiser la recherche de similarit√© entre les embeddings. Ensuite, nous lisons le fichier `embeddings.csv` pour extraire les segments de texte ainsi que leurs embeddings, que nous convertissons en un tableau NumPy de type `float32` (format attendu par `faiss`). Nous cr√©ons un index FAISS de type `IndexFlatL2`, bas√© sur la distance euclidienne (L2), et nous y ins√©rons les embeddings. Enfin, cet index est sauvegard√© dans un fichier `faiss_index.bin`, ce qui nous permettra d'effectuer des recherches rapides sur les phrases du corpus.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eff746fc-063a-46c2-b522-5ae58c9d0f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index FAISS sauvegard√© dans faiss_index.bin\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "\n",
    "df = pd.read_csv(\"embeddings.csv\")\n",
    "chunks = df[\"chunk\"].tolist()\n",
    "embeddings = np.array(df[\"embedding\"].apply(eval).tolist()).astype('float32')\n",
    "\n",
    "index = faiss.IndexFlatL2(embeddings.shape[1])  \n",
    "index.add(embeddings)\n",
    "\n",
    "faiss.write_index(index, \"faiss_index.bin\")\n",
    "\n",
    "print(\"Index FAISS sauvegard√© dans faiss_index.bin\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8834d902-8564-4509-a786-4ceec4ae61bc",
   "metadata": {},
   "source": [
    "# 3 - Recherche de similarit√©\n",
    "\n",
    "Nous chargeons uniquement la colonne `chunk` du fichier `embeddings.csv`, qui contient les segments de texte et nous r√©cup√©rons l'index √† partir du fichier `faiss_index.bin` pour effectuer des recherches directement sur les vecteurs enregistr√©s.\n",
    "La fonction `rechercher_similaires(query, top_k=5)` encode la requ√™te en un vecteur `float32` et le compare aux vecteurs de l‚Äôindex FAISS √† l‚Äôaide de `.search()`, qui renvoie les indices des `top_k` segments les plus proches ainsi que leurs distances.\n",
    "Enfin, nous testons le syst√®me avec la requ√™te **\"Quel est le lien entre IA et philosophie ?\"**. La fonction retourne les cinq segments les plus similaires avec leurs scores de distance, un score plus bas indiquant une plus grande proximit√© avec la requ√™te.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b345cc81-ae58-4213-b37a-e5bc4d1d4fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 1.0322 - Texte: D√©fis et √âthique\n",
      "\n",
      "Malgr√© ses nombreux avantages, l'IA pose √©galement des d√©fis importants\n",
      "Score: 1.1991 - Texte: En comprenant son historique, ses applications, et ses d√©fis, nous pouvons mieux appr√©hender son potentiel et ses implications futures\n",
      "Score: 1.2376 - Texte: Dans la finance, elle est utilis√©e pour d√©tecter les fraudes et optimiser les investissements\n",
      "Score: 1.2548 - Texte: Applications de l'IA\n",
      "\n",
      "L'IA est aujourd'hui utilis√©e dans divers secteurs, tels que la sant√©, la finance, et le divertissement\n",
      "Score: 1.2887 - Texte: Conclusion\n",
      "\n",
      "L'intelligence artificielle est un domaine en pleine expansion qui promet de transformer notre monde de mani√®re significative\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "chunks = pd.read_csv(\"embeddings.csv\", usecols=[\"chunk\"])[\"chunk\"].tolist()\n",
    "\n",
    "index = faiss.read_index(\"faiss_index.bin\")\n",
    "\n",
    "def rechercher_similaires(query, top_k=5):\n",
    "    query_embedding = model.encode([query]).astype('float32')  \n",
    "    distances, indices = index.search(query_embedding, top_k)  \n",
    "    \n",
    "    resultats = [(chunks[idx], distances[0][i]) for i, idx in enumerate(indices[0]) if idx != -1]\n",
    "    return resultats\n",
    "\n",
    "query = \"Quel est le lien entre IA et philosophie ?\"\n",
    "resultats = rechercher_similaires(query)\n",
    "\n",
    "for texte, score in resultats:\n",
    "    print(f\"Score: {score:.4f} - Texte: {texte}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074cd88e-ec6a-4490-a6b0-4fbccf06b144",
   "metadata": {},
   "source": [
    "# 4 - G√©n√©ration d'une r√©ponse avec un LLM (OpenAI)\n",
    "\n",
    "## üîç 4.1. Recherche des passages similaires  \n",
    "La fonction `rechercher_similaires(query, top_k=5)` permet de retrouver les segments de texte les plus proches d‚Äôune requ√™te donn√©e :  \n",
    "- La requ√™te est convertie en embedding √† l‚Äôaide du mod√®le `SentenceTransformer`.  \n",
    "- L‚Äôindex FAISS est utilis√© pour rechercher les `top_k` passages les plus similaires en fonction de la distance.  \n",
    "- La fonction retourne une liste des segments correspondants avec leurs scores de similarit√©.  \n",
    "\n",
    "## üìù 4.2. Recherche et g√©n√©ration de r√©ponse  \n",
    "- L‚Äôutilisateur pose une question, ici **\"cite le passage relatif √† l'√©thique et son titre ?\"**.  \n",
    "- La fonction `rechercher_similaires()` extrait les passages pertinents du corpus.  \n",
    "- Ces passages sont combin√©s en un **contexte textuel**.  \n",
    "- OpenAI (`gpt-3.5-turbo`) g√©n√®re une r√©ponse √† partir de ce contexte.  \n",
    "- La r√©ponse est affich√©e √† l‚Äôutilisateur.  \n",
    "\n",
    "üöÄ **Ce syst√®me permet d‚Äôextraire rapidement des passages pertinents et de g√©n√©rer des r√©ponses pr√©cises √† partir d‚Äôun texte.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea2ba97c-4584-4f57-bc3d-1e54d7744fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R√©ponse g√©n√©r√©e :\n",
      "Le passage relatif √† l'√©thique dans les extraits que vous avez fournis est le suivant :\n",
      "\"Il est crucial de d√©velopper des cadres √©thiques pour guider l'utilisation responsable de l'IA\"\n",
      "Ce passage se situe dans la section intitul√©e \"D√©fis et √âthique\".\n"
     ]
    }
   ],
   "source": [
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "client = openai.OpenAI(api_key=OPENAI_API_KEY)\n",
    "\n",
    "model = SentenceTransformer(\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "chunks = pd.read_csv(\"embeddings.csv\", usecols=[\"chunk\"])[\"chunk\"].tolist()\n",
    "\n",
    "index = faiss.read_index(\"faiss_index.bin\")\n",
    "\n",
    "def rechercher_similaires(query, top_k=5):\n",
    "    query_embedding = model.encode([query]).astype('float32')\n",
    "    distances, indices = index.search(query_embedding, top_k)\n",
    "    \n",
    "    resultats = [(chunks[idx], distances[0][i]) for i, idx in enumerate(indices[0]) if idx != -1]\n",
    "    return resultats\n",
    "\n",
    "def generer_reponse(context, question):\n",
    "    prompt = f\"Voici des extraits pertinents : {context}\\n\\nR√©ponds √† cette question : {question}\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Tu es un assistant utile.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "query = \"cite le passage relatif √† l'√©thique et son titre ?\"\n",
    "resultats = rechercher_similaires(query)\n",
    "\n",
    "context = \"\\n\".join([texte for texte, _ in resultats])\n",
    "\n",
    "reponse = generer_reponse(context, query)\n",
    "print(\"R√©ponse g√©n√©r√©e :\")\n",
    "print(reponse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ef56ebb-de71-40bf-8659-dfcf29fe9d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
